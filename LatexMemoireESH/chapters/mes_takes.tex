\section{Des programmeurs-économistes forgés par les sciences "dures" et l’informatique rudimentaire, porteuses de logiciels ambitieux}

Les logiciels qui composent notre corpus naissent relativement tôt dans l'histoire et le développement de l'informatique moderne. PcGive de Hendry est conçu sur les premiers PC, à partir de programmes initialement écrits pour les ordinateurs mainframes. Ainsi, alors que l'informatique en tant qu'outil technique est en train d'être mise au point, la création de logiciels et l'informatisation de l'économie débute déjà.


Les entretiens analysés montrent un fait frappant : les créateurs de logiciels en économie ne sont pas formés en premier lieu comme économistes.

Ils viennent majoritairement des sciences dites "dures" ou "exactes" : mathématiques, informatique, ingénierie ou physique. Et il semble que cela soit également vrai au-delà de notre corpus, comme nous avons pu le voir avec les exemples de Dynare et OpenFisca.
Ce constat peu paraitre étonnant au premier abord, les logiciels produits par ces programmeurs sont tous destinés à la recherche en économie. Mais il n'a en fait rien de surprenant. L’informatique est historiquement issue des mathématiques, de la physique et de l’ingénierie comme l'explique Matti Tedre, historien et philosophe de l'informatique dans son livre \textit{"The Science of Computing: Shaping a Discipline"}\cite{tedreScienceComputingShaping2014}. De plus son enseignement a été, et reste aujourd’hui, institutionnellement rattaché à ces disciplines, alors que l’économie est classée parmi les sciences humaines et sociales.

À titre d'exemple, on peut constater que :
\begin{itemize}
  \item A l’université Paris 1, l’enseignement et la recherche en mathématique et informatique sont regroupés au sein de l’UFR 27, tandis que les sciences économiques forment l’UFR02.
  \item A l’université Paris Saclay, la licence d’économie est logée au sein de l’UFR de Droit, Économie et Gestion, tandis que la licence d’informatique se trouve au sein de la Faculté des sciences regroupant Mathématiques, Informatique, Physique, Chimie, Biologie et Sciences de la Terre.
  \item A la Sorbonne Université (fusion de Paris IV et Paris VI) l’informatique est logée dans la faculté des sciences et ingénierie, les Sciences humaines au sens large (pas de licence d’économie) sont regroupées dans la faculté des lettres.
  \item A Oxford, le département d'informatique appartient à la \textit{Mathematical, Physical and Life Sciences Division} tandis que celui d'économie appartient à la \textit{Social Sciences Division}
\end{itemize}

Il est donc normal que les premiers logiciels destinés à l’économie, que le processus d’informatisation de l’économie, ait été portée par des personnes issues d'autres disciplines. Mais ayant acquis, du fait de leurs parcours académique, des compétences poussées de programmation.

Ce découpage institutionnel reflète une division du travail : les compétences en programmation sont acquises ailleurs, et importées ensuite en économie. Il n’est donc pas étonnant que les premiers logiciels économiques aient été développés par des profils hybrides, programmeurs avant d’être économistes.



Ces trajectoires individuelles s’inscrivent toutes à une période où la programmation est encore une pratique nouvelle et en plein essor. Cette génération a donc appris à maitriser cette technologie dans un cadre rudimentaire.

Cette nouvelle technologie est alors porteuse de grandes promesses mais les contraintes matérielles sont encore lourdes. Les machines disposent de très peu de mémoire, beaucoup de logiciels ne disposent pas d'interfaces graphiques\footnote{Pour les utiliser, il faut alors utiliser une interface en ligne de commande (CLI). C'est le cas de PcGive dont on peut tester le fonctionnement sur le museum numérique de Jurgen Doornik: \url{https://museumofeconometrics.org.doornik.com/}}, et les langages de programmation restent rudimentaires (Fortran, Pascal, BASIC). L’apprentissage exige une compréhension profonde du fonctionnement des machines et l'utilisation de langage de bas niveau, produisant ainsi des programmeurs particulièrement à l’aise techniquement — bien plus que les générations suivantes, formées dans des environnements plus conviviaux. Une littérature journalistique et scientifique s'attache d'ailleurs à analyser ce phénomène de pertes de compétences des \textit{digital natives}. On peut citer à ce sujet cet article de Kirschner et De Bruyckere \cite{kirschnerMythsDigitalNative2017} et cet article de Georgia Wells dans le Wall Street Journal\cite{wellsGenZersAre2024}.

Ce constat rejoint un paradoxe bien documenté : contrairement à l’idée répandue du « digital native », l’exposition précoce aux technologies ne garantit ni compétence, ni autonomie critique. L’ICILS 2018 (International Computer and Information Literacy Study) montre par exemple que seuls 2\% des étudiants évalués atteignent le niveau le plus élevé de littératie numérique, c’est-à-dire la capacité à utiliser l’informatique de manière créative et critique pour produire de l’information nouvelle.

La majorité reste cantonnée à un usage élémentaire, et à peine 19\% des élèves sont capables d’utiliser les technologies de façon autonome pour rechercher, évaluer et gérer l’information \footnote{ICILS 2018, International Computer and Information Literacy Study, IEA, \url{https://www.iea.nl/studies/iea/icils/2018}}. Et ce phénomène touche aussi la population des programmeurs. Voilà ce que l'on peut lire dans le rapport de 2018 sur les compétences des développeurs publié par HackerRank\cite{2018DeveloperSkills} :

\begin{quote}
\begin{center}
\textit{"Unlike generations thereafter, if kids of the seventies wanted to see innovative technology, they’d have to build it themselves — they had no other choice. There were no widespread resources to teach them how to build software. Almost half of all developers (47\%) between the ages of 45 and 54 started coding before they were 16 years old. Meanwhile, developers between 18 and 24 today are the least likely to have started coding before 16 (only 20\%).
Developers between the ages of 45 and 54 were among the first to get their hands on relatively powerful PCs, like the Acorn Archimedes, TRS-80, Commodore 64, and Apple II. With limited to no access to formal education, young people in the PC Revolution had an unusually strong drive to learn to code on their own."}
\end{center}
\end{quote} \hfill Extrait du Developer Skills Report de 2018, HackerRank\cite{2018DeveloperSkills}

Ce contexte a donc favorisé un apprentissage exigeant mais formateur. Ceux qui ont traversé cette phase d’appropriation sont devenus particulièrement à l’aise avec l’outil informatique. Ils ont acquis une maîtrise profonde de la programmation, bien plus poussée que celle des générations suivantes, formées dans des environnements déjà stabilisés et “conviviaux”.



Cette double caractéristique des programmeurs-économistes, formation en sciences "dures" et apprentissage dans un environnement technique contraint, explique qu'ils aient pu créer des logiciels d’une grande complexité à une époque où il faut presque tout construire. D'ailleurs, dans plusieurs entretiens, les programmeurs-économistes témoignent avoir passé beaucoup de temps à résoudre des problèmes très techniques et très éloignés de l'aspect purement économique des logiciels mais apportant beaucoup de valeur à leurs logiciels. Axtell témoigne par exemple qu'en l'absence d'outils dédiés à la visualisation, Epstein et lui on investi plusieurs jours et plusieurs milliers de dollars dans du matériel d'accélération vidéo. 

Fischbacher, dans le cadre du développement de z-Tree a passé énormément de temps à intégrer des fonctionnalités de réseau pour permettre l’interaction en temps réel entre participants d’expériences. Les programmeurs de WinSolve et PcGive ont consacré une attention particulière aux interfaces graphiques pour rendre l’outil accessible aux utilisateurs. Agnès Gramain enfin, a dû réécrire elle-même des algorithmes de base d’économétrie, comme celui de maximisation de la vraisemblance \footnote{Page 5 de l'entretien \cite{gramainInterviewAgnesGramain2024}}, aujourd'hui préprogrammé et disponible sur tous les logiciels d'économétrie.

Ces témoignages illustrent combien la valeur de ces logiciels ne tient pas seulement à leur contenu économique, mais aussi à la capacité de leurs concepteurs à surmonter des obstacles techniques fondamentaux, en dotant leurs outils de fonctionnalités inédites qui ont largement conditionné leur adoption et leur pérennité.










\section{Le passage des programmeurs-économistes aux économistes-programmeurs}


Mais aujourd’hui, l’écosystème informatique des économistes a profondément changé. Il n’est plus nécessaire d'avoir une maîtrise poussée des langages de bas niveau. Les langages de haut niveau comme R ou Python offrent des environnements beaucoup plus faciles à prendre en main pour des économistes de formation, non formés à l'ingénierie logicielle.
De nombreuses bibliothèques de codes sont disponibles, offrant à l'économiste une boite à outils très développés et optimisés par des informaticiens de métier et la communauté des usagers. Ce temps de travail que l'économiste n'a pas à fournir pour créer ses outils informatique, il peut alors le consacrer aux tâches dans lesquels il a la plus grande valeur ajoutée : la modélisation économique, la production de résultats empiriques et l’élaboration de nouvelles méthodes adaptées aux questions de recherche de l'économie. L'existence d'économistes-programmeurs est rendue possible par ces nouveaux outils.

Cette évolution s’accompagne d’un changement dans la manière de créer des outils. Alors que les pionniers devaient concevoir des logiciels entiers, des interfaces entre ces logiciels et leurs utilisateurs, parfois même des langages, la tendance actuelle est de développer des bibliothèques spécialisées, constituées de quelques dizaines de fonctions, destinées à répondre à un problème précis. Ces bibliothèques s’intègrent dans de vastes écosystèmes open source comme CRAN pour R ou PyPI pour Python. Elles bénéficient de la relecture et de l’appui d’une large communauté, ce qui limite les risques de bogues et garantit une certaine standardisation. Ce déplacement réduit les risques que des oublis ou des erreurs, inhérents au développement logiciel, compromettent des résultats. Mais il pose désormais un autre défi : se distinguer dans une offre pléthorique de packages, non plus en luttant contre l’invisibilité, mais en gagnant en visibilité au sein d’un marché saturé d’outils.

Cette situation modifie le profil des développeurs de logiciel en économie aujourd'hui, qui se sont adapté à ces changements. Les économistes-programmeurs d’aujourd’hui sont avant tout des économistes, et non des programmeurs venus d'autres disciplines. Leur compétence première est l'économie, et la programmation est un instrument secondaire qui peut être mobilisé lorsque l'étude d'une question économique le commande.


Cette forte recomposition de l'écosystème informatique des économistes a eu lieu au cours des deux dernières décennies et tous les dévelopeurs-économistes de notre corpus sont donc trop anciens pour les avoir vécus (Turocy a soutenu son doctorat en 2001). Nous pensons donc qu'il serait pertinent d’élargir ce corpus en menant des entretiens avec des économistes-programmeurs plus jeunes. Clément de Chaisemartin, par exemple, qui a obtenu son doctorat en 2013, a développé plusieurs packages économétriques, disponibles sur les logiciels R et Stata, destinés à l’estimation d’effets de traitement hétérogènes et à l’évaluation de politiques publiques\footnote{Voir \url{https://github.com/chaisemartinPackages}}, et constitue à ce titre un profil particulièrement intéressant.
On pourrait également s’intéresser aux équipes de l’Institut des politiques publiques (IPP) travaillant sur le logiciel de microsimulation TAXIPP. L’étude de ces chercheurs et de leurs pratiques permettrait de mieux comprendre comment la microsimulation, technique intrinsèquement liée à l’informatique, contribue à renforcer (ou non) le rôle de « conseiller du prince » des économistes. En effet, cette méthode s’est imposée au cœur des administrations françaises et y occupe une place très importante. Elle est notamment utilisée pour anticiper l’évolution des recettes et des dépenses liée aux impôts, taxes, cotisations et prestations de sécurité sociale.










\section{Toujours à la frontière de l'innovation, entre promesses et risques d’abstraction}


Un autre élément qui ressort de notre enquête est le sentiment permanent, partagé entre les générations de programmeurs-économistes, d’évoluer à la frontière de l’innovation. Comme l’écrit Renfro dans son article rétrospectif de 2004, \textit{Econometric Software: The first Fifty Years in Perspective} \cite{renfroEconometricSoftwareFirst2004}, 

\begin{quote}
\begin{center}
\textit{"The period of development of econometric software spans the professional careers of almost all working economists. At each step along the road, what constituted “the present” at that time seemed to offer both promise and the sense of being at the forefront.”}
\end{center}
\end{quote} \hfill \textit{Econometric Software: The first Fifty Years in Perspective}, page 58 \cite{renfroEconometricSoftwareFirst2004}

 
Cette impression était déjà présente dans les années 1970, lorsque la programmation se faisait sur des ordinateurs centraux ou sur des premiers PC encore balbutiants. Elle demeure aujourd’hui, même si l’environnement a radicalement changé : à chaque époque, l’informatisation de l’économie s’accompagne de l’idée que l’on se trouve à l’avant-garde technologique.

Les développements récents des grands modèles de langage (LLM) accentuent encore cette dynamique. Après être passés des langages bas niveau (Fortran, Cobol) aux langages de haut niveau (R, Python), les économistes peuvent désormais interagir avec la machine dans un langage proche du langage naturel. Les environnements de “vibe coding” ou l’usage de copilotes de programmation basés sur l’IA promettent d’abaisser encore la barrière technique. La génération actuelle dispose ainsi d’outils qui rendent la programmation plus accessible que jamais, au prix d’un éloignement toujours plus grand de la couche technique. Là où Hendry, Gramain ou Axtell devaient comprendre les algorithmes en profondeur, leurs successeurs peuvent aujourd’hui obtenir du code fonctionnel à partir d’une simple instruction en langage courant.

Mais cette nouvelle couche d’abstraction ne va pas sans risques. D’une part, les technologies d’intelligence artificielle appliquées à la programmation sont encore immatures, et leur fiabilité reste incertaine. D’autre part, elles renforcent une inquiétude déjà exprimée par Johnston il y a 45 ans et rappelée par Renfro dans son article de 2004 : celle d’une utilisation aveugle d’outils puissants, sans réelle compréhension de leurs fondements. Comme Johnston le soulignait en 1991, 


\begin{quote}
\begin{center}
\textit{“It is thus all too possible for someone to activate an econometric software package, of which he has only a dim understanding, to apply it to data of whose nature and provenance he is ignorant, and then to draw conclusions about an economic situation, whose historical and institutional realities he has, perhaps, not studied in any depth.”}
\end{center}
\end{quote} \hfill \textit{Econometrics Retrospect and Prospect }, page 2 \cite{johnstonEconometricsRetrospectProspect1991}


Le danger est bien celui d’une déconnexion entre l’économiste et les données, entre l’utilisateur et l’outil. L’innovation technologique, en offrant des solutions de plus en plus “clé en main”, facilite l’accès à l’informatique mais peut aussi fragiliser la capacité critique de ceux qui s’en servent.


